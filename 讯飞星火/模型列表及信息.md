> 2025-7-16 23:32:00

### 模型信息总览

| 模型名称 | 任务类型 | 标签 | 模型描述 | 开发者 | 更新时间 | 操作 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **Qwen2.5-VL-32B-Instruct** | 图像理解 | `中文`, `32k` | Qwen2.5-VL 是基于 Qwen2-VL 反馈打造的最新视觉语言模型，在多方面实现关键增强：视觉理解上，能分析图像中的文本、图表等复杂内容；具备代理能力，可作为视觉代理推理并动态调用工具实现电脑、手机操作；能通过生成边界框、点等实现不同格式的视觉定位，还支持发票、表格等扫描数据的结构化输出。 | @阿里巴巴 | 2025-07-05 | `API调用`, `体验` |
| **Qwen3-32B** | 文本生成 | `多语言`, `8k`, `企业级推理` | Qwen3-32B 是 Qwen 系列中大型因果语言模型，经预训练与后期优化，具备 328 亿参数规模，采用 64 层 Transformer 架构，原生支持 32K tokens 长上下文（通过 YaRN 技术可扩展至 131K tokens），兼顾复杂任务处理能力与长文本场景适配，适用于企业级推理、多轮对话及长文档分析等需求。 | @阿里巴巴 | 2025-05-12 | `精调`, `体验`, `API调用` |
| **Qwen3-14B** | 文本生成 | `多语言`, `16k`, `企业级推理` | Qwen3-14B 是 Qwen 系列中大型因果语言模型，经预训练与后期优化，具备 148 亿参数规模，采用 40 层 Transformer 架构，原生支持 32K tokens 长上下文（通过 YaRN 技术可扩展至 131K tokens），兼顾模型性能与长文本处理能力，适用于企业级推理、多轮对话及长文档分析等任务。 | @阿里巴巴 | 2025-07-05 | `精调`, `体验`, `API调用` |
| **Qwen3-8B** | 文本生成 | `多语言`, `16k`, `指令遵循` | Qwen3-8B 是 Qwen 系列中大型因果语言模型，经预训练与后期优化，具备 82 亿参数规模，采用 36 层 Transformer 架构，原生支持 32K tokens 长上下文（通过 YaRN 技术可扩展至 131K tokens），兼顾模型效率与长文本处理能力，适用于企业级推理、多轮对话及长文档分析等任务。 | @阿里巴巴 | 2025-05-12 | `精调`, `体验`, `API调用` |
| **Qwen3-4B** | 文本生成 | `多语言`, `16k`, `轻量级` | Qwen3-4B 是 Qwen 系列的轻量级因果语言模型，经预训练与后期优化，具备 40 亿参数规模，采用 36 层 Transformer 架构，原生支持 32K tokens 长上下文（通过 YaRN 技术可扩展至 131K tokens），兼顾高效推理与长文本处理能力，适用于中小规模场景的快速部署与多轮对话、文档分析等任务。 | @阿里巴巴 | 2025-07-05 | `精调`, `体验`, `API调用` |
| **Qwen3-1.7B** | 文本生成 | `多语言`, `32k`, `轻量级` | Qwen3-1.7B 是 Qwen 系列轻量级因果语言模型，经预训练与后期优化，具备 17 亿参数规模，采用 28 层 Transformer 架构，原生支持 32,768 tokens 长上下文，兼顾高效推理与长文本处理能力，适合中小算力场景快速部署及多轮对话、短文本生成等任务。 | @阿里巴巴 | 2025-07-05 | `精调`, `体验`, `API调用` |
| **Qwen3-0.6B** | 文本生成 | `多语言`, `32k`, `指令遵循`, `意图识别` | Qwen3-0.6B 是 Qwen 系列轻量级因果语言模型，经预训练与后期优化，具备 60 亿参数规模，采用 28 层 Transformer 架构，原生支持 32,768 tokens 长上下文，适合中小算力场景快速部署及轻量级对话、短文本生成等任务。 | @阿里巴巴 | 2025-07-05 | `精调`, `API调用` |
| **Qwen3-235B-A22B** | 文本生成 | `多语言`, `32k`, `MoE`, `指令遵循` | Qwen3-235B-A22B是Qwen系列新一代因果语言模型，基于混合专家（MoE）架构构建，通过预训练与后期优化，在推理能力、指令遵循、代理工具整合及多语言支持上实现前沿突破。 | @阿里巴巴 | 2025-07-05 | `API调用`, `体验` |
| **Qwen3-30B-A3B** | 文本生成 | `多语言`, `32k`, `MoE`, `逻辑推理` | Qwen3-30B-A3B 是 Qwen 系列新一代因果语言模型，基于混合专家（MoE）架构，通过预训练与后期优化，实现推理能力、任务适配与多语言支持的全面突破。 | @阿里巴巴 | 2025-07-05 | `API调用`, `体验` |
| **星火医疗大模型-Lite** | 文本生成 | `中文`, `8k`, `医疗` | 讯飞医疗自研的医疗垂类大模型，是针对医疗领域需求开发的 AI 模型。它经海量医疗数据预训练，可辅助医疗知识问答、药物查询、提供健康管理建议等。相比通用模型，其对医疗知识挖掘更深，应用更贴合场景。 | @科大讯飞 | 2025-07-05 | `精调` |
| **Qwen2-72B-Instruct** | 文本生成 | `多语言`, `131k`, `指令遵循`, `逻辑推理` | Qwen2-72B-Instruct 是 Qwen2 大语言模型系列中的 720 亿参数指令调优模型，支持最长 131,072 tokens 上下文长度，可处理超长文本输入。其在语言理解、生成、多语言、编码、数学推理等多领域基准测试中表现突出。 | @阿里巴巴 | 2025-04-14 | `API调用`, `体验` |
| **FLUX.1-dev** | 文生图 | `英文`, `非商业` | FLUX.1 [dev] 是由 Black Forest Labs 开发的 120 亿参数校正流变换器文生图模型，能够根据文本描述高效生成高分辨率图像。生成内容遵循非商业许可协议（CC-BY-NC 4.0），支持个人、学术及受限商业用途。 | @Black Forest Labs | 2025-07-05 | `API调用`, `体验` |
| **Kolors** | 文生图 | `多语言` | Kolors是快手Kolors团队开发的基于潜在扩散的文本到图像生成模型。它在数十亿文本-图像对数据上训练，在视觉质量、复杂语义理解、中英文字体渲染方面优势明显，支持中英双语输入，对中文内容处理能力强，能高效生成高质量图像。 | @快手 | 2025-07-05 | `API调用`, `体验` |
| **Wan2.1-I2V-14B-480P** | 视频生成 | `多语言`, `影视制作` | Wan2.1-I2V-14B-480P 是阿里云通义万相系列的开源图生视频模型，拥有140亿参数，能基于输入图像生成480P分辨率视频。 | @阿里巴巴 | 2025-07-05 | `体验` |
| **Wan2.1-T2V-1.3B** | 视频生成 | `多语言`, `影视制作` | Wan2.1-T2V-1.3B是阿里云通义万相2.1系列中的轻量化文本转视频生成模型，参数规模为13亿，支持生成480P分辨率（854×480）的动态视频。 | @阿里巴巴 | 2025-07-05 | `体验` |
| **QwQ-32B** | 文本生成 | `英文`, `32k`, `逻辑推理` | QwQ是Qwen系列的推理模型。与传统的指令调优模型相比，具备思考和推理能力的QwQ能够在下游任务中实现显著增强的性能表现，尤其是在解决复杂难题方面。 | @阿里巴巴 | 2025-03-06 | `API调用`, `体验` |
| **DeepSeek-R1-Distill-Qwen-32B** | 文本生成 | `多语言`, `8k`, `深度思考` | DeepSeek-R1-Distill-Qwen-32B 是基于 DeepSeek-R1推理数据蒸馏训练的模型，基础模型为Qwen2.5-32B。在数学推理、代码生成等复杂任务中表现卓越，尤其擅长多步推导和跨领域问题解决。 | @深度求索 | 2025-02-08 | `精调`, `体验`, `API调用` |
| **DeepSeek-R1-Distill-Llama-8B** | 文本生成 | `多语言`, `8k` | DeepSeek-R1-Distill-Llama-8B 是基于 DeepSeek-R1推理数据蒸馏训练的模型，基础模型为Llama-3.1-8B。在数学推理、代码生成等任务中表现卓越。 | @深度求索 | 2025-02-08 | `一键部署` |
| **DeepSeek-R1-Distill-Qwen-14B** | 文本生成 | `多语言`, `8k` | DeepSeek-R1-Distill-Qwen-14B 是基于 DeepSeek-R1推理数据蒸馏训练的模型，基础模型为Qwen2.5-14B。在数学推理、代码生成等复杂任务中表现卓越，尤其擅长多步推导和跨领域问题解决。 | @深度求索 | 2025-02-08 | `精调`, `体验`, `API调用` |
| **DeepSeek-R1-Distill-Qwen-7B** | 文本生成 | `多语言`, `8k`, `深度思考`, `逻辑推理` | DeepSeek-R1-Distill-Qwen-7B 是基于 DeepSeek-R1推理数据蒸馏训练的模型，基础模型为Qwen2.5-Math-7B。在数学推理、代码生成等复杂任务中表现卓越，尤其擅长多步推导和跨领域问题解决。 | @深度求索 | 2025-02-08 | `精调`, `体验`, `API调用` |
| **DeepSeek-R1-Distill-Qwen-1.5B** | 文本生成 | `多语言`, `8k` | DeepSeek-R1-Distill-Qwen-1.5B 是基于 DeepSeek-R1推理数据蒸馏训练的模型，基础模型为Qwen2.5-Math-1.5B。具备优秀的数学推理与代码生成能力。 | @深度求索 | 2025-02-08 | `精调`, `体验` |
| **DeepSeek-V3** | 文本生成 | `多语言`, `32k` | DeepSeek-V3 是一款由深度求索公司自研的MoE模型。DeepSeek-V3 多项评测成绩超越了 Qwen2.5-72B 和 Llama-3.1-405B 等其他开源模型，并在性能上和世界顶尖的闭源模型 GPT-4o 以及 Claude-3.5-Sonnet 不分伯仲。 | @深度求索 | 2025-07-05 | `精调`, `体验`, `API调用` |
| **DeepSeek-R1** | 文本生成 | `多语言`, `32k`, `深度思考` | 该版本通过利用更多的计算资源并在后训练阶段引入算法优化机制，显著提升了其推理能力和深度推理能力。该模型在数学、编程和一般逻辑等多个基准测试中表现出色，整体性能已接近 O3 和 Gemini 2.5 Pro 等领先模型。 | @深度求索 | 2025-07-07 | `精调`, `体验`, `API调用` |
| **Qwen_v2.5_7b_base** | 文本生成 | `多语言`, `128k`, `知识库`, `指令遵循`, `代码` | 【可商用】Qwen_v2.5_7b_base是由阿里巴巴集团Qwen团队研发的大型语言模型。拥有76.1亿参数，支持超过29种语言，支持长达128K token的输入，在编程和数学领域表现出较强的知识和能力。 | @阿里巴巴 | 2025-01-22 | `精调` |
| **Qwen_v2.5_3b_base** | 文本生成 | `多语言`, `32k`, `代码`, `数学` | 【可商用】Qwen_v2.5_3b_base是由阿里巴巴集团Qwen团队研发的Qwen2.5系列大型语言模型。支持超过29种语言，支持长达32,768 token的输入，在自然语言理解、代码编写、数学解题等方面都有显著增强。 | @阿里巴巴 | 2025-01-14 | `精调` |
| **Qwen_v2.5_1.5b_base** | 文本生成 | `多语言`, `32k`, `代码`, `数学` | 【可商用】Qwen_v2.5_1.5b_base是由阿里巴巴集团Qwen团队研发的Qwen2.5系列大型语言模型。拥有15.4亿参数，支持超过29种语言，支持长达32,768 token的输入，适合于资源受限的环境。 | @阿里巴巴 | 2025-01-14 | `精调` |
| **Qwen_v2.5_0.5b_base** | 文本生成 | `多语言`, `32k`, `指令遵循`, `意图识别` | 【可商用】Qwen_v2.5_0.5b_base是Qwen_v2.5系列中最小的模型，具有5.1亿参数。支持长达32,768 token的输入，具有较快的处理速度和较低的资源消耗，适合对响应时间和资源消耗有严格要求的应用场景。 | @阿里巴巴 | 2025-01-14 | `精调` |
| **Spark Lite Patch** | 文本生成 | `多语言`, `8k`, `内容创作`, `逻辑推理` | 讯飞自研的轻量级大语言模型，具有更高的推理性能并兼顾优异的模型效果。适用于对时延要求极高的移动设备端快速响应场景，例如移动APP中的快速问答、智能语音助手的基础交互等场景。 | @科大讯飞 | 2025-06-13 | `精调`, `体验`, `API调用` |
| **Qwen_v2.5_14b_Instruct** | 文本生成 | `多语言`, `128k` | 【可商用】Qwen2.5-14b 是阿里qwen团队推出的超大规模语言模型，支持 128k 长上下文及多语言，在数学、编码、指令跟踪等多方面能力显著增强，适用于多种应用场景 。 | @阿里巴巴 | 2025-01-22 | `精调`, `API调用` |
| **internlm2.5_7b_chat** | 文本生成 | `多语言`, `8k`, `仅供研究` | 【仅供研究】internlm2.5_7b_chat是InternLM2.5开源的70亿个参数的基础模型并为实际场景量身定制的聊天模型。旨在理解和生成自然语言文本。 | @书生·浦语 | 2024-11-05 | `精调` |
| **internlm2.5_1.8b_chat** | 文本生成 | `多语言`, `8k`, `仅供研究` | 【仅供研究】internlm2.5_1.8b_chat是InternLM2.5开源的18亿个参数的基础模型并为实际场景量身定制的聊天模型，旨在理解和生成自然语言文本。 | @书生·浦语 | 2024-11-05 | `精调` |
| **Bert_base_chinese** | 文本分类 | `中文`, `8k`, `语义分析` | Bert_base_chinese 是一个由 Google 开发的基于中文的预训练语言模型。能够用于多种自然语言处理任务，如文本分类、命名实体识别、情感分析等。 | @google | 2024-10-23 | `精调` |
| **Chinese_roberta_wwm_ext** | 文本分类 | `中文`, `8k`, `语义分析` | Chinese_roberta_wwm_ext 是一个针对中文自然语言处理加速而开发的预训练模型。采用了 Whole Word Masking（全词掩码）技术，提高了模型对中文词汇语义理解的准确性和效率。 | @google | 2024-10-23 | `精调` |
| **Spark Max** | 文本生成 | `多语言`, `8k`, `内容创作`, `逻辑推理` | 讯飞自研的专业级大语言模型，在内容创作和知识处理相关场景中表现卓越。适用于对内容质量和知识专业性要求高的业务场景，如高端内容创作、专业知识服务等。 | @科大讯飞 | 2024-10-22 | `精调`, `体验`, `API调用` |
| **Qwen_v2.5_7b_Instruct** | 文本生成 | `多语言`, `128k`, `指令遵循`, `代码` | 【可商用】Qwen_v2.5_7b_Instruct是由阿里巴巴集团Qwen团队研发的大型语言模型。拥有76.1亿参数，支持超过29种语言，支持长达128K token的输入，适用于智能客服、个人助理和教育辅助工具等场合。 | @阿里巴巴 | 2025-07-05 | `精调`, `体验`, `API调用` |
| **Qwen_v2.5_3b_Instruct** | 文本生成 | `多语言`, `32k`, `代码`, `数学` | 【可商用】Qwen_v2.5_3b_Instruct是由阿里巴巴集团Qwen团队研发的Qwen2.5系列大型语言模型。支持超过29种语言，支持长达32,768 token的输入，在指令执行、生成长文本、理解结构化数据等方面取得了显著改进。 | @阿里巴巴 | 2024-11-05 | `精调` |
| **Qwen_v2.5_1.5b_Instruct** | 文本生成 | `多语言`, `32k`, `代码`, `数学` | 【可商用】Qwen_v2.5_1.5b_Instruct是由阿里巴巴集团Qwen团队研发的Qwen2.5系列大型语言模型。拥有15.4亿参数，支持超过29种语言，支持长达32,768 token的输入，适合于资源受限的环境。 | @阿里巴巴 | 2024-11-05 | `精调` |
| **Qwen_v2.5_0.5b_Instruct** | 文本生成 | `多语言`, `32k`, `指令遵循`, `意图识别` | 【可商用】Qwen_v2.5_0.5b_Instruct是Qwen_v2.5系列中最小的模型，具有5.1亿参数。支持长达32,768 token的输入，非常适合对响应时间和资源消耗有严格要求的应用场景。 | @阿里巴巴 | 2024-11-05 | `精调` |
| **Spark Tiny** | 文本生成 | `多语言`, `8k` | 讯飞自研的超高性能大语言模型，其部署和精调成本在星火系列模型中最具性价比。适用于小型企业客服、低成本智能交互应用等场景。 | @科大讯飞 | 2025-02-07 | `精调` |
| **Spark Mini** | 文本生成 | `多语言`, `8k`, `逻辑推理` | 讯飞自研的高性能大语言模型，适合作为基座模型进行精调，能出色地处理较为复杂的特定场景问题。适用于企业智能客服、专业知识咨询等场景。 | @科大讯飞 | 2024-10-22 | `精调` |
| **Spark Mini Instruct** | 文本生成 | `多语言`, `8k`, `指令遵循` | 讯飞自研的高性能大语言模型，在指令解析等场景上有明显优势。适用于工业控制、复杂设备交互等场景。 | @科大讯飞 | 2024-10-22 | `精调` |
| **Spark Lite** | 文本生成 | `多语言`, `8k`, `内容创作`, `逻辑推理` | 讯飞自研的轻量级大语言模型，具有更高的推理性能并兼顾优异的模型效果。适用于移动APP中的快速问答、智能语音助手的基础交互等场景。 | @科大讯飞 | 2024-10-22 | `精调`, `体验`, `API调用` |
| **Gemma2_9b_it** | 文本生成 | `英文`, `8k`, `知识问答`, `逻辑推理` | Gemma 是谷歌推出的轻量级先进开源模型家族，适用于多种文本生成任务，如问答、摘要和推理。由于其相对较小的尺寸，可以部署在资源有限的环境中。 | @google | 2024-10-22 | `精调`, `API调用` |
| **Gemma_2b_it** | 文本生成 | `英文`, `8k`, `知识问答`, `逻辑推理` | Gemma 是谷歌推出的轻量级先进开源模型家族，适用于多种文本生成任务，如问答、摘要和推理。由于其相对较小的尺寸，可以部署在资源有限的环境中。 | @google | 2024-12-26 | `精调` |
| **Qwen_v2_0.5b_Instruct** | 文本生成 | `多语言`, `8k`, `内容创作` | 【可商用】Qwen是阿里巴巴集团Qwen团队研发的大语言模型和大型多模态模型系列。Qwen具备自然语言理解、文本生成、视觉理解、音频理解、工具使用、角色扮演、作为AI Agent进行互动等多种能力。 | @阿里巴巴 | 2024-11-05 | `精调` |
| **Qwen_v2_1.5b_Instruct** | 文本生成 | `多语言`, `8k`, `内容创作` | 【可商用】Qwen是阿里巴巴集团Qwen团队研发的大语言模型和大型多模态模型系列。Qwen具备自然语言理解、文本生成、视觉理解、音频理解、工具使用、角色扮演、作为AI Agent进行互动等多种能力。 | @阿里巴巴 | 2024-11-05 | `精调` |
| **Starcode2_3b** | 文本生成 | `英文`, `16k`, `代码` | StarCoder2，由BigCode 与NVIDIA 合作，是面向开发者的非常先进的代码LLM。您可以使用模型的功能快速构建应用程序，包括代码完成、自动填充、高级代码摘要以及使用自然语言检索相关代码片段。 | @BigCode | 2024-10-21 | `精调` |
| **StableDiffusion_XL_Base_1** | 文生图 | `英文` | 【可商用】SD XL 1.0由Stability AI研发并开源的文生图模型。 | @Stability AI | 2024-12-10 | `精调`, `体验`, `API调用` |
| **Vision Transformer (ViT)** | 图像分类 | `英文` | Vision Transformer (ViT) 是一种Transformer编码器模型（类似 BERT），以监督方式对大量图像进行预训练，即 ImageNet-21k，分辨率为 224x224 像素。 | @google | 2024-10-21 | `精调` |